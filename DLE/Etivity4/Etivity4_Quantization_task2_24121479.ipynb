{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d4325ad",
   "metadata": {},
   "source": [
    "# Your Details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633d99a8",
   "metadata": {},
   "source": [
    "Your Name: Dylan Rodrigues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b8d888",
   "metadata": {},
   "source": [
    "Your ID Number: 24121479"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d67540",
   "metadata": {},
   "source": [
    "# Etivity Task 4 - Part 2: Quantizing a TensorFlow/Keras Model\n",
    "\n",
    "For this exercise, you will apply various quantization strategies to a convolutional neural network (CNN) trained on the Fashion MNIST dataset. The first section of this exercise is already completed (Sections 1 and 2). Your task is to perform various quantizations on this model uses the TF Model optimisations toolkit and report on the results with your own code in Sections 3, 4 and 5.\n",
    "\n",
    "By the end of this notebook, you'll be able to: \n",
    "\n",
    "* Understand Quantizations in TensorFlow \n",
    "* Quantize a CNN using the TensorFlow Model optimisation framework\n",
    "* Analyse the model perfromance\n",
    "* Results analysis\n",
    "\n",
    "### Let's get started!\n",
    "**Start** with sections [1] and [2] for which code is provided - then proceed with sections [3], [4] and [5] to begin this model quantization exercise.\n",
    "\n",
    "    [1] Import data dependencies\n",
    "    [2] Generate a TensorFlow/keras CNN model for the Fashion MNIST dataset\n",
    "    [3] Convert model to TF Lite model\n",
    "    [4] Perform Post Training Quantization (PTQ) to generate TF Lite model for:\n",
    "        (a) PTQ using Float 16 Quantization\n",
    "        (b) PTQ using Dynamic Range Quantization\n",
    "        (c) PTQ using Full Integer (int8) Quantization \n",
    "        (d) Evaluate the TF Lite models\n",
    "    [5] Perform Quantization Aware Training (QAT)\n",
    "        (a) Train a TF model through tf.keras\n",
    "        (b) Make it quantization-aware\n",
    "        (c) Quantize the model using Dynamic Range Quantization\n",
    "        (d) Evaluate the TF Lite model performance\n",
    "    \n",
    "   \n",
    "### Important Note on Submission \n",
    "\n",
    "There are code exercises to complete in this task.  Insert your code entries into the cell areas marked with the 'enter code here' text as below, so that grading can easily be assessed.\n",
    "\n",
    "\\### **ENTER CODE HERE**\n",
    "\n",
    "Please make sure you are not doing the following:\n",
    "\n",
    "1. You have not added any _extra_ `print` statement(s) in the assignment.\n",
    "2. You have not added any _extra_ code cell(s) in the assignment.\n",
    "3. You have not changed any of the function parameters.\n",
    "4. You are not using any global variables inside your graded exercises. Unless specifically instructed to do so, please refrain from it and use the local variables instead.\n",
    "5. You are not changing the assignment code where it is not required, like creating _extra_ variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b741ce1e",
   "metadata": {},
   "source": [
    "### Installing the TensorFlow Model Optimisation toolkit\n",
    "\n",
    "You must first install it using pip (comment this out once you have done this).\n",
    "\n",
    "<span style='color: red;'>**Note:**</span> There is no need to run this command again if used ok from the previous tutorial. (Hence commented out here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c331999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the TF optimization toolkit the first time \n",
    "#! pip install -q tensorflow-model-optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a6e6be",
   "metadata": {},
   "source": [
    "## 1. Import the data dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8aa85c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow \n",
    "import time\n",
    "import os\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96ed78b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "# Check that we are using a GPU\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(physical_devices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234be510",
   "metadata": {},
   "source": [
    "## 2. Generate a TensorFlow Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56141d13",
   "metadata": {},
   "source": [
    "We'll build a CNN model to classify the 10 fashion item categories from the [FASHION_MNIST dataset](https://www.tensorflow.org/datasets/catalog/fashion_mnist).\n",
    "\n",
    "This training won't take long because you're training the model for just 5 epochs, which trains to about ~90% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6735ca55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\24121479\\.conda\\envs\\cpu_env\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7624 - loss: 0.6511 - val_accuracy: 0.8723 - val_loss: 0.3501\n",
      "Epoch 2/5\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.8787 - loss: 0.3279 - val_accuracy: 0.8967 - val_loss: 0.2822\n",
      "Epoch 3/5\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.8958 - loss: 0.2763 - val_accuracy: 0.9027 - val_loss: 0.2667\n",
      "Epoch 4/5\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9103 - loss: 0.2417 - val_accuracy: 0.9025 - val_loss: 0.2622\n",
      "Epoch 5/5\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9189 - loss: 0.2147 - val_accuracy: 0.9093 - val_loss: 0.2530\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1aabd3ce540>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Fashion MNIST dataset\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Reshape data for CNN input\n",
    "img_width, img_height = 28, 28\n",
    "X_train = X_train.reshape(X_train.shape[0], img_width, img_height, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_width, img_height, 1)\n",
    "input_shape = (img_width, img_height, 1)\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 to 1.\n",
    "X_train = X_train.astype(np.float32) / 255.0\n",
    "X_test = X_test.astype(np.float32) / 255.0\n",
    "\n",
    "\n",
    "# Define the model architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Dropout(rate=0.1), # Randomly disable 10% of neurons\n",
    "    tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Dropout(rate=0.1), # Randomly disable 10% of neurons\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "# Build the model\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.sparse_categorical_crossentropy, # loss function\n",
    "    optimizer=tf.keras.optimizers.Adam(), # optimizer function\n",
    "    metrics=['accuracy'] # reporting metric\n",
    ")\n",
    "\n",
    "# Train the fashion MNIST classification model\n",
    "model.fit(\n",
    "  X_train,\n",
    "  y_train,\n",
    "  epochs=5,\n",
    "  validation_split=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5c7951",
   "metadata": {},
   "source": [
    "**Evaluate and save the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8a6b8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9046 - loss: 0.2671\n",
      "Test loss 0.2629, accuracy 90.52%\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(\"Test loss {:.4f}, accuracy {:.2f}%\".format(score[0], score[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1854b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "#Save the entire model into a model.h5 file\n",
    "model.save(\"models/model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef9444e",
   "metadata": {},
   "source": [
    "## 3. Convert the trained model to TensorFlow Lite format\n",
    "\n",
    "In the code cell below, convert the model to a **TensorFlow Lite** model and then save this unquantized TFLite model to the ./fashion_mnist_tflite_model directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec207baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\24121479\\AppData\\Local\\Temp\\tmpi15_a892\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\24121479\\AppData\\Local\\Temp\\tmpi15_a892\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\24121479\\AppData\\Local\\Temp\\tmpi15_a892'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1832830871760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1832830871376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1832830870800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1832830869072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1832830867152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1832830870608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1832830872144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1832830872528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1832839214096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1832839210832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1825276"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### ENTER CODE HERE\n",
    "model = tf.keras.models.load_model('models/model.h5')\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "import pathlib\n",
    "tflite_models_dir = pathlib.Path(\"./mnist_tflite_models/\")\n",
    "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Save the unquantized float model:\n",
    "tflite_model_file = tflite_models_dir/\"mnist_model.tflite\"\n",
    "tflite_model_file.write_bytes(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad0ab56",
   "metadata": {},
   "source": [
    "It's now a TensorFlow Lite model, but it's still using 32-bit float values for all parameter data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5111131b",
   "metadata": {},
   "source": [
    "## 4. Post-Training Quantization (PTQ)\n",
    "\n",
    "### Part (a): PTQ using Float 16 Quantization\n",
    "Here you will insert code for post-training float 16 quantization and then evaluate the file size compared to the unquantized tflite model size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e949f36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\24121479\\AppData\\Local\\Temp\\tmpf07uelvm\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\24121479\\AppData\\Local\\Temp\\tmpf07uelvm\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\24121479\\AppData\\Local\\Temp\\tmpf07uelvm'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1832830869456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1832830866384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1832839210064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1832839207184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1832839209296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1832839207760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1832839215632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1832839215824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1832839215248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1832839216208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "915704"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('models/model.h5')\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "tflite_quant16_model = converter.convert()\n",
    "\n",
    "# Info: Save the quantized 16-bit model\n",
    "tflite_quant16_model_file = tflite_models_dir/\"mnist_model_quant16.tflite\"\n",
    "tflite_quant16_model_file.write_bytes(tflite_quant16_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a76ac94",
   "metadata": {},
   "source": [
    "**Evaluate the reduction in size of the model** - how much smaller is the Quantized 16-bit model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2f04e939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float model in Mb: 1.7407188415527344\n",
      "Quantized 16-bit model in Mb: 0.8732833862304688\n",
      "Compression ratio: 1.9933035129255743\n"
     ]
    }
   ],
   "source": [
    "print(\"Float model in Mb:\", os.path.getsize(tflite_model_file) / float(2**20))\n",
    "print(\"Quantized 16-bit model in Mb:\", os.path.getsize(tflite_quant16_model_file) / float(2**20))\n",
    "print(\"Compression ratio:\", os.path.getsize(tflite_model_file)/os.path.getsize(tflite_quant16_model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ae4ad9",
   "metadata": {},
   "source": [
    "### Part (b): PTQ using Dynamic Range Quantization\n",
    "Next you will quantize the original model dynamically to change the model weight and activations from float to int8 format. Convert the model using **Dynamic Range Quantization** and evaluate the model file size reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0db93f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\24121479\\AppData\\Local\\Temp\\tmpgbxl3_0s\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\24121479\\AppData\\Local\\Temp\\tmpgbxl3_0s\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\24121479\\AppData\\Local\\Temp\\tmpgbxl3_0s'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1832874300944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1832874308240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1832874309008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1832874306896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1832874307088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1832874305744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1832874304400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1832874305552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1832874304592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1832874310352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "469432"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('models/model.h5')\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "# Info: Save the quantized model\n",
    "tflite_quant_model_file = tflite_models_dir/\"mnist_model_quant.tflite\"\n",
    "tflite_quant_model_file.write_bytes(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8446084",
   "metadata": {},
   "source": [
    " **Evaluate the reduction in size of the model** - how much smaller is the Quantized model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "176f70b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float model in Mb: 1.7407188415527344\n",
      "Quantized model in Mb: 0.44768524169921875\n",
      "Compression ratio: 3.8882649670239777\n"
     ]
    }
   ],
   "source": [
    "print(\"Float model in Mb:\", os.path.getsize(tflite_model_file) / float(2**20))\n",
    "print(\"Quantized model in Mb:\", os.path.getsize(tflite_quant_model_file) / float(2**20))\n",
    "print(\"Compression ratio:\", os.path.getsize(tflite_model_file)/os.path.getsize(tflite_quant_model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54f0646",
   "metadata": {},
   "source": [
    "### Part (c): PTQ using Full Integer (int8) Quantization \n",
    "Convert the original model to satisfy **full integer quantization** so that everything is converted (including activations) from float32 into int8 format. Evaluate the model file size reduction. Note you will need to use the OPTIMIZE_FOR_SIZE option by using a small representative dataset of the model and also make sure the input and output tensors are in int8 format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f102c970",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\24121479\\AppData\\Local\\Temp\\tmpdk29la6_\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\24121479\\AppData\\Local\\Temp\\tmpdk29la6_\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\24121479\\AppData\\Local\\Temp\\tmpdk29la6_'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1834045648016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1834045645136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1834045646864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1834045650128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1834045649936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1834045651280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1834045651472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1834045652240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1834045651856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1834045650704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\24121479\\.conda\\envs\\cpu_env\\Lib\\site-packages\\tensorflow\\lite\\python\\convert.py:997: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "472568"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def representative_data_gen():\n",
    "  for input_value in tf.data.Dataset.from_tensor_slices(X_train).batch(1).take(100):\n",
    "    yield [input_value]\n",
    "\n",
    "model = tf.keras.models.load_model('models/model.h5')\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "tflite_fullquant_model = converter.convert()\n",
    "\n",
    "# Saving the fully-quantized 8-bit model:\n",
    "tflite_fullquant_model_file = tflite_models_dir/\"mnist_model_fullquant.tflite\"\n",
    "tflite_fullquant_model_file.write_bytes(tflite_fullquant_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607223a6",
   "metadata": {},
   "source": [
    "**Check that the input and output tensors are in int8 format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "106b9677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  <class 'numpy.uint8'>\n",
      "output:  <class 'numpy.uint8'>\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=tflite_fullquant_model)\n",
    "input_type = interpreter.get_input_details()[0]['dtype']\n",
    "print('input: ', input_type)\n",
    "output_type = interpreter.get_output_details()[0]['dtype']\n",
    "print('output: ', output_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0aaa4a8",
   "metadata": {},
   "source": [
    " **Evaluate the reduction in size of the model** - how much smaller is the Quantized model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "aa8c9ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float model in Mb: 1.7407188415527344\n",
      "Full Integer Quantized model in Mb: 0.45067596435546875\n",
      "Compression ratio: 3.862462121853363\n"
     ]
    }
   ],
   "source": [
    "print(\"Float model in Mb:\", os.path.getsize(tflite_model_file) / float(2**20))\n",
    "print(\"Full Integer Quantized model in Mb:\", os.path.getsize(tflite_fullquant_model_file) / float(2**20))\n",
    "print(\"Compression ratio:\", os.path.getsize(tflite_model_file)/os.path.getsize(tflite_fullquant_model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33fbc8c",
   "metadata": {},
   "source": [
    "### Part (d):  Evaluate the TF Lite models on all images\n",
    "\n",
    "In this section, evaluate the four TF Lite models by running inference using the TensorFlow Lite [`Interpreter`](https://www.tensorflow.org/api_docs/python/tf/lite/Interpreter) to compare the model accuracies. First, build a **run_tflite_model()** function to run inference on a TF Lite model and then an **evaluate_model()** function to evaluate the TF Lite model on all images in the X_test dataset.\n",
    "\n",
    "**Evaluate the model performance for these models** by reporting on the model accuracies.\n",
    "1. Float model (Unquantized)\n",
    "2. 16-bit quantized model\n",
    "3. Initial quantized 8-bit model\n",
    "4. Fully quantized 8-bit model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "521bf4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run inference on a TFLite model\n",
    "def run_tflite_model(tflite_file, test_image_indices):\n",
    "  global X_test\n",
    "\n",
    "  # Initialize the interpreter\n",
    "  interpreter = tf.lite.Interpreter(model_path=str(tflite_file))\n",
    "  interpreter.allocate_tensors()\n",
    "\n",
    "  input_details = interpreter.get_input_details()[0]\n",
    "  output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "  predictions = np.zeros((len(test_image_indices),), dtype=int)\n",
    "  for i, test_image_index in enumerate(test_image_indices):\n",
    "    test_image = X_test[test_image_index]\n",
    "    test_label = y_test[test_image_index]\n",
    "\n",
    "    # Check if the input type is quantized, then rescale input data to uint8\n",
    "    if input_details['dtype'] == np.uint8:\n",
    "      input_scale, input_zero_point = input_details[\"quantization\"]\n",
    "      test_image = test_image / input_scale + input_zero_point\n",
    "\n",
    "    test_image = np.expand_dims(test_image, axis=0).astype(input_details[\"dtype\"])\n",
    "    interpreter.set_tensor(input_details[\"index\"], test_image)\n",
    "    interpreter.invoke()\n",
    "    output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
    "\n",
    "    predictions[i] = output.argmax()\n",
    "\n",
    "  return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "de9a2b3f-ec96-484a-9fee-a376b51d9d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to evaluate a TFLite model on all images\n",
    "def evaluate_model(tflite_file, model_type):\n",
    "  global X_test\n",
    "  global y_test\n",
    "\n",
    "  test_image_indices = range(X_test.shape[0])\n",
    "  predictions = run_tflite_model(tflite_file, test_image_indices)\n",
    "\n",
    "  accuracy = (np.sum(y_test== predictions) * 100) / len(X_test)\n",
    "\n",
    "  print('%s model accuracy is %.4f%% (Number of test samples=%d)' % (\n",
    "      model_type, accuracy, len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606b4d3d",
   "metadata": {},
   "source": [
    "1. Evaluate the float model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "eacfd86c-48f7-4f90-b84e-d2b4eb7b11af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('mnist_tflite_models/mnist_model.tflite')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4776ce1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float model accuracy is 90.5200% (Number of test samples=10000)\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(tflite_model_file, model_type=\"Float\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc534c5",
   "metadata": {},
   "source": [
    "2. Evaluate the 16-bit quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bda7ec42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16-bit Quantized model accuracy is 90.5000% (Number of test samples=10000)\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(tflite_quant16_model_file, model_type=\"16-bit Quantized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9154c7b3",
   "metadata": {},
   "source": [
    "3. Evaluate the initial quantized 8-bit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2fdb55ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model accuracy is 90.5500% (Number of test samples=10000)\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(tflite_quant_model_file, model_type=\"Quantized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1967e30a",
   "metadata": {},
   "source": [
    "4. Evaluate the fully quantized 8-bit integer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "20c4d68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fully Quantized model accuracy is 90.4300% (Number of test samples=10000)\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(tflite_fullquant_model_file, model_type=\"Fully Quantized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844b8c7c",
   "metadata": {},
   "source": [
    "## 5. Quantization-Aware Training (QAT)\n",
    "\n",
    "QAT models quantization during training and typically provides higher accuracies as compared to post-training quantization. \n",
    "Generally, QAT is a three-step process:\n",
    "\n",
    "    (a) Train a regular model through tf.keras \n",
    "        YOU MAY HAVE TO 'import tf_keras as keras' and use model = keras.Sequential([...]) format.\n",
    "    (b) Make it quantization-aware by applying the related API, allowing it to learn those loss-robust parameters.\n",
    "    (c) Quantize the model use one of the approaches mentioned above and analyse performance\n",
    "\n",
    "\n",
    "### **Part (a)**: Train a model for the FASHION MNIST dataset again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7917f6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\24121479\\.conda\\envs\\cpu_env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\24121479\\.conda\\envs\\cpu_env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\24121479\\.conda\\envs\\cpu_env\\Lib\\site-packages\\tf_keras\\src\\backend.py:1400: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\24121479\\.conda\\envs\\cpu_env\\Lib\\site-packages\\tf_keras\\src\\backend.py:1400: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\24121479\\.conda\\envs\\cpu_env\\Lib\\site-packages\\tf_keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\24121479\\.conda\\envs\\cpu_env\\Lib\\site-packages\\tf_keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\24121479\\.conda\\envs\\cpu_env\\Lib\\site-packages\\tf_keras\\src\\optimizers\\__init__.py:317: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\24121479\\.conda\\envs\\cpu_env\\Lib\\site-packages\\tf_keras\\src\\optimizers\\__init__.py:317: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:From C:\\Users\\24121479\\.conda\\envs\\cpu_env\\Lib\\site-packages\\tf_keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\24121479\\.conda\\envs\\cpu_env\\Lib\\site-packages\\tf_keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\24121479\\.conda\\envs\\cpu_env\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\24121479\\.conda\\envs\\cpu_env\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.2844 - accuracy: 0.9203 - val_loss: 0.1134 - val_accuracy: 0.9688\n",
      "Epoch 2/5\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.1123 - accuracy: 0.9679 - val_loss: 0.0818 - val_accuracy: 0.9758\n",
      "Epoch 3/5\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0832 - accuracy: 0.9756 - val_loss: 0.0708 - val_accuracy: 0.9805\n",
      "Epoch 4/5\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0703 - accuracy: 0.9793 - val_loss: 0.0645 - val_accuracy: 0.9822\n",
      "Epoch 5/5\n",
      "1688/1688 [==============================] - 3s 2ms/step - loss: 0.0619 - accuracy: 0.9816 - val_loss: 0.0699 - val_accuracy: 0.9803\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x1ab08462600>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 to 1.\n",
    "train_images = train_images.astype(np.float32) / 255.0\n",
    "test_images = test_images.astype(np.float32) / 255.0\n",
    "\n",
    "# Define the model architecture\n",
    "#model = tf.keras.Sequential([\n",
    "#  tf.keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "#  tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "#  tf.keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
    "#  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "#  tf.keras.layers.Flatten(),\n",
    "#  tf.keras.layers.Dense(10)\n",
    "#])\n",
    "\n",
    "import tf_keras as keras\n",
    "\n",
    "model = keras.Sequential([\n",
    "  keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "  keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "  keras.layers.Conv2D(filters=12, kernel_size=(3, 3), activation='relu'),\n",
    "  keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "  keras.layers.Flatten(),\n",
    "  keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "\n",
    "# Train the digit classification model\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                  from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(\n",
    "  train_images,\n",
    "  train_labels,\n",
    "  epochs=5,\n",
    "  validation_split=0.1\n",
    "  #validation_data=(test_images, test_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9ad1bc38-43a3-462a-8a18-d31459ca55c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# Save the entire model into a model.h5 file\n",
    "model.save(\"models/model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbc778d",
   "metadata": {},
   "source": [
    "### Part (b): Make the model quantization aware\n",
    "Hint: Use q_aware_model = quantize_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "05faf889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quantize_layer (QuantizeLa  (None, 28, 28)            3         \n",
      " yer)                                                            \n",
      "                                                                 \n",
      " quant_reshape (QuantizeWra  (None, 28, 28, 1)         1         \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      " quant_conv2d (QuantizeWrap  (None, 26, 26, 12)        147       \n",
      " perV2)                                                          \n",
      "                                                                 \n",
      " quant_max_pooling2d (Quant  (None, 13, 13, 12)        1         \n",
      " izeWrapperV2)                                                   \n",
      "                                                                 \n",
      " quant_flatten (QuantizeWra  (None, 2028)              1         \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      " quant_dense (QuantizeWrapp  (None, 10)                20295     \n",
      " erV2)                                                           \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20448 (79.88 KB)\n",
      "Trainable params: 20410 (79.73 KB)\n",
      "Non-trainable params: 38 (152.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "\n",
    "# q_aware stands for quantization aware.\n",
    "q_aware_model = quantize_model(model)\n",
    "\n",
    "# `quantize_model` requires a recompile.\n",
    "q_aware_model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                  from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "q_aware_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e475cc",
   "metadata": {},
   "source": [
    "#### Retrain the quantization aware model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1661628c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1688/1688 [==============================] - 5s 2ms/step - loss: 0.0564 - accuracy: 0.9832 - val_loss: 0.0619 - val_accuracy: 0.9845\n",
      "Epoch 2/5\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.0502 - accuracy: 0.9847 - val_loss: 0.0631 - val_accuracy: 0.9840\n",
      "Epoch 3/5\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.0458 - accuracy: 0.9855 - val_loss: 0.0613 - val_accuracy: 0.9840\n",
      "Epoch 4/5\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.0423 - accuracy: 0.9872 - val_loss: 0.0627 - val_accuracy: 0.9833\n",
      "Epoch 5/5\n",
      "1688/1688 [==============================] - 4s 2ms/step - loss: 0.0391 - accuracy: 0.9881 - val_loss: 0.0580 - val_accuracy: 0.9835\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x1ab08a339e0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_aware_model.fit(\n",
    "  train_images,\n",
    "  train_labels,\n",
    "  epochs=5,\n",
    "  validation_split=0.1\n",
    "  #validation_data=(test_images, test_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080913b4",
   "metadata": {},
   "source": [
    "#### Compare the accuracy of the baseline model to the new QAT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "521e4ca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 957us/step - loss: 0.0784 - accuracy: 0.9760\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0577 - accuracy: 0.9817\n",
      "Baseline test accuracy: 97.60000109672546\n",
      "Quant test accuracy: 98.17000031471252\n"
     ]
    }
   ],
   "source": [
    "_, baseline_model_accuracy = model.evaluate(\n",
    "    test_images, test_labels, verbose=1)\n",
    "\n",
    "_, q_aware_model_accuracy = q_aware_model.evaluate(\n",
    "    test_images, test_labels, verbose=1)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy*100)\n",
    "print('Quant test accuracy:', q_aware_model_accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1242bf",
   "metadata": {},
   "source": [
    "#### Fine tune with QAT on a subset of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "707dd9da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "29/29 [==============================] - 0s 4ms/step - loss: 0.0441 - accuracy: 0.9889 - val_loss: 0.0243 - val_accuracy: 0.9900\n",
      "Epoch 2/5\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0237 - accuracy: 0.9933 - val_loss: 0.0301 - val_accuracy: 0.9900\n",
      "Epoch 3/5\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0156 - accuracy: 0.9956 - val_loss: 0.0273 - val_accuracy: 0.9900\n",
      "Epoch 4/5\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0109 - accuracy: 0.9989 - val_loss: 0.0287 - val_accuracy: 0.9800\n",
      "Epoch 5/5\n",
      "29/29 [==============================] - 0s 3ms/step - loss: 0.0079 - accuracy: 1.0000 - val_loss: 0.0304 - val_accuracy: 0.9900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf_keras.src.callbacks.History at 0x1ab05cb1a30>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_aware_model.fit(\n",
    "  train_images[:1000],\n",
    "  train_labels[:1000],\n",
    "  epochs=5,\n",
    "  validation_split=0.1\n",
    "  #validation_data=(test_images, test_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d218cb3",
   "metadata": {},
   "source": [
    "#### Re-evaluate the model accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b2db3c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 961us/step - loss: 0.0784 - accuracy: 0.9760\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0601 - accuracy: 0.9791\n",
      "Baseline test accuracy: 97.60000109672546\n",
      "Quant test accuracy: 97.9099988937378\n"
     ]
    }
   ],
   "source": [
    "_, baseline_model_accuracy = model.evaluate(\n",
    "    test_images, test_labels, verbose=1)\n",
    "\n",
    "_, q_aware_model_accuracy = q_aware_model.evaluate(\n",
    "    test_images, test_labels, verbose=1)\n",
    "\n",
    "print('Baseline test accuracy:', baseline_model_accuracy*100)\n",
    "print('Quant test accuracy:', q_aware_model_accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f5d686",
   "metadata": {},
   "source": [
    "#### Save the QAT model to the ./models directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b36edf7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\24121479\\.conda\\envs\\cpu_env\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py:3098: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native TF-Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the entire model into a qat_model.h5 file\n",
    "# Note: The previous model has already been saved as model.h5; now after customizing it, im saving the model in qat_model.h5\n",
    "model.save(\"models/qat_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f4a6b9",
   "metadata": {},
   "source": [
    "### Part (c): Convert the model to TF Lite format  using Dynamic Range Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e6cf8e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "WARNING:absl:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\24121479\\AppData\\Local\\Temp\\tmprnvgt0ni\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\24121479\\AppData\\Local\\Temp\\tmprnvgt0ni\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\24121479\\AppData\\Local\\Temp\\tmprnvgt0ni'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28), dtype=tf.float32, name='input_1')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1834045637840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1834088026512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1834088026320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1834088028048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24016"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('models/qat_model.h5')\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quantaware_model = converter.convert()\n",
    "\n",
    "# Saving the quantized aware model:\n",
    "tflite_quantaware_model_file = tflite_models_dir/\"mnist_model_quantaware.tflite\"\n",
    "tflite_quantaware_model_file.write_bytes(tflite_quantaware_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e32eae",
   "metadata": {},
   "source": [
    "**Evaluate the reduction in size of the model.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f50137a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Float model in Mb: 0.09896087646484375\n",
      "Quantized aware (QAT) model in Mb: 0.0229034423828125\n",
      "Compression ratio: 4.3207861425716185\n"
     ]
    }
   ],
   "source": [
    "print(\"Float model in Mb:\", os.path.getsize(\"models/model.h5\") / float(2**20))\n",
    "print(\"Quantized aware (QAT) model in Mb:\", os.path.getsize(tflite_quantaware_model_file) / float(2**20))\n",
    "print(\"Compression ratio:\", os.path.getsize(\"models/model.h5\")/os.path.getsize(tflite_quantaware_model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82990f54",
   "metadata": {},
   "source": [
    "### Part (d): Evaluate the TF Lite QAT model accuracy\n",
    "Hint: Use the intrepreter evaluate_model() function to get the accuracy result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "809710e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_model(interpreter):\n",
    "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "  # Run predictions on every image in the \"test\" dataset.\n",
    "  prediction_digits = []\n",
    "  for i, test_image in enumerate(test_images):\n",
    "    if i % 1000 == 0:\n",
    "      print('Evaluated on {n} results so far.'.format(n=i))\n",
    "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
    "    # the model's input data format.\n",
    "    test_image = np.expand_dims(test_image, axis=0).astype(np.float32)\n",
    "    interpreter.set_tensor(input_index, test_image)\n",
    "\n",
    "    # Run inference.\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Post-processing: remove batch dimension and find the digit with highest\n",
    "    # probability.\n",
    "    output = interpreter.tensor(output_index)\n",
    "    digit = np.argmax(output()[0])\n",
    "    prediction_digits.append(digit)\n",
    "\n",
    "  print('\\n')\n",
    "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "  prediction_digits = np.array(prediction_digits)\n",
    "  accuracy = (prediction_digits == test_labels).mean()\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "29a6132a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated on 0 results so far.\n",
      "Evaluated on 1000 results so far.\n",
      "Evaluated on 2000 results so far.\n",
      "Evaluated on 3000 results so far.\n",
      "Evaluated on 4000 results so far.\n",
      "Evaluated on 5000 results so far.\n",
      "Evaluated on 6000 results so far.\n",
      "Evaluated on 7000 results so far.\n",
      "Evaluated on 8000 results so far.\n",
      "Evaluated on 9000 results so far.\n",
      "\n",
      "\n",
      "Quant TFLite test_accuracy: 0.9761\n",
      "Quant TF test accuracy: 0.9790999889373779\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=tflite_quantaware_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "test_accuracy = evaluate_model(interpreter)\n",
    "\n",
    "print('Quant TFLite test_accuracy:', test_accuracy)\n",
    "print('Quant TF test accuracy:', q_aware_model_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ab6f8c",
   "metadata": {},
   "source": [
    "## <span style='color: red;'>Comment on the results of this exercise:</span> ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b49b34c",
   "metadata": {},
   "source": [
    "The results of this Quantization-Aware Training (QAT) exercise demonstrate that QAT improves model accuracy while significantly reducing model size. The baseline model achieved a test accuracy of **97.6%**, while the QAT model slightly improved to **98.17%**, showing that quantization-aware training helps maintain accuracy. Fine-tuning on a subset of data resulted in a minor accuracy drop to **97.91%**, which is still competitive. After converting to a TF Lite model with dynamic range quantization, the model size was reduced by over **4.3x** (from ~0.099MB to ~0.023MB), demonstrating efficient compression. The quantized TFLite model achieved a **97.61%** accuracy, closely matching the QAT-trained TensorFlow model (**97.91%**), proving that quantization had minimal impact on performance while significantly reducing the model footprint. This suggests that QAT is an effective approach for deploying models in resource-constrained environments while maintaining high accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cpu_env)",
   "language": "python",
   "name": "cpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
