{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your Name: Dylan Rodrigues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your ID Number: 24121479"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etivity 2 - Task 1: Hyperparamter Tuning - Improving your Deep Neural Network!\n",
    "\n",
    "You will need to have completed Task 2 of Etivity 1! in order to do this assignment.\n",
    "\n",
    "The aim of this task is to improve your dog vs. cat DNN model for which you'll use the functions from the previous assignment to optimise a deep layer neural network using hyperparameter tuning. There are 3 main exercise parts to this task.\n",
    "\n",
    "1. Apply weight parameter initialisations to the learning process.\n",
    "2. Use L2 regularization in your deep learning model.\n",
    "3. Apply dropout regularization.\n",
    "\n",
    "Training your neural network requires specifying an initial value of the weights. A well-chosen initialisation method helps the learning process. In this notebook, you'll try out a few different initialisations, including random, zeros, and He initialisation, and see how each leads to different results.\n",
    "\n",
    "Deep Learning models have so much flexibility and capacity that **overfitting can be a serious problem**, if the training dataset is not big enough, it does well on the training set, but the learned network **doesn't generalise to new examples** that it has never seen!\n",
    "\n",
    "**For this assignment you will be need to complete the following exercises:**\n",
    "\n",
    "- Section 3: [Exercise 1-1](#ex-1), [Exercise 1-2](#ex-2), [Exercise 1-3](#ex-3): Analyze various initialisation methods to speed up the convergence of gradient descent and to increase the odds of gradient descent converging to a lower training. (and generalisation) error \n",
    "- Section 4: [Exercise 2-1](#ex-4), [Exercise 2-2](#ex-5): Implement L2 regularization in your model to help overcome overfitting.\n",
    "- Section 5: [Exercise 3-1](#ex-6), [Exercise 3-2](#ex-7): Evaluate Droprout regularization in your model.\n",
    "\n",
    "\n",
    "## Important Note on Submission \n",
    "\n",
    "There are 3 main exercises to complete in this task. Use notes provided to help you build understanding. Insert your code entries for each of these exercises. Please make sure to enter your code after this line so that grading can easily be assessed.\n",
    "\n",
    "**\\# ENTER CODE HERE**\n",
    "\n",
    "\n",
    "Please make sure you are not doing the following:\n",
    "\n",
    "1. You have not added any _extra_ `print` statement(s) in the assignment.\n",
    "2. You have not added any _extra_ code cell(s) in the assignment.\n",
    "3. You have not changed any of the function parameters.\n",
    "4. You are not using any global variables inside your graded exercises. Unless specifically instructed to do so, please refrain from it and use the local variables instead.\n",
    "5. You are not changing the assignment code where it is not required, like creating _extra_ variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [1 - Packages](#1)\n",
    "- [2 - Load and Process the Dataset](#2)\n",
    "- [3 - L layer Model Neural Network- Parameter Initialisation](#3)\n",
    "    - [3.1 - Random Initialisation](#3-1)\n",
    "        - [Exercise 1.1 - initialise_parameters_random](#ex-1)\n",
    "    - [3.2 - Xavier Initialisation](#3-2)\n",
    "        - [Exercise 1.2 - initialise_parameters_xavier](#ex-2)\n",
    "    - [3.3 - He Initialisation](#3-3)\n",
    "        - [Exercise 1.3 - initialise_parameters_he](#ex-3)\n",
    "- [4 - L-layer Neural Network with L2 regularization](#4)\n",
    "    - [4.1 - Regularized Model](#4-1)\n",
    "    - [4.2 - L2 Regularization](#4-2)\n",
    "        - [Exercise 2.1 - compute_cost_with_regularization](#ex-4)\n",
    "        - [Exercise 2.2 - linear_backward_with_regularization](#ex-5)        \n",
    "- [5 - L-layer Neural Network with Dropout](#5)\n",
    "    - [5.1 - Dropout Model](#5-1)\n",
    "    - [5.2 - Forward Propagation with Dropout](#5-1)\n",
    "        - [Exercise 3.1 - drop_out_matrices and L_model_forward_with_dropout](#ex-6)\n",
    "    - [5.3 - Backward Propagation with Dropout](#5-1)\n",
    "        - [Exercise 3.2 - L_model_backward_with_dropout](#ex-7)\n",
    "- [6 - Results Analysis](#6)\n",
    "- [7 - Test with your own image (optional/ungraded exercise)](#7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by importing all the packages you'll need during this assignment. \n",
    "\n",
    "- [numpy](https://www.numpy.org/) is the fundamental package for scientific computing with Python.\n",
    "- [matplotlib](http://matplotlib.org) is a library to plot graphs in Python.\n",
    "- [h5py](http://www.h5py.org) is a common package to interact with a dataset that is stored on an H5 file.\n",
    "- [PIL](http://www.pythonware.com/products/pil/) and [scipy](https://www.scipy.org/) are used here to test your model with your own picture at the end.\n",
    "- [pandas](https://pandas.pydata.org/) is a powerful, flexible and easy to use open source data analysis and manipulation tool that also allos us to plot data visually.\n",
    "- [os](https://docs.python.org/3/library/os.html)  This package provides a portable way of using operating system dependent functionality.\n",
    "- `dnn_fucntions` provides the encrypted functions implemented in the \"Improving your Deep Neural Network: Step by Step\" assignment to this notebook. You will need to  copy the \"dist\" folder into your directory structure.\n",
    "- `np.random.seed(1)` is used to keep all the random function calls consistent. It helps grade your work - so please don't change it! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ndimage\n\u001b[0;32m---> 11\u001b[0m os\u001b[38;5;241m.\u001b[39mchdir (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdist\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdnn_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     13\u001b[0m os\u001b[38;5;241m.\u001b[39mchdir (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dist'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "os.chdir (\"dist\")\n",
    "from dnn_functions import *\n",
    "os.chdir (\"..\")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Dylan/Documents/UL/DLE/Etivity2\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2 - Load and Process the Dataset\n",
    "\n",
    "You'll load the exam same datset that you used in Activity 1 - Part 2 for the Cat vs. Dog classification exercise. Recall that we only have 1250 images in total. For this exercise, we will assign 75% of the images for the training set and 25% of the images for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Etivity2_DNN_regularisation_24121479.ipynb\n",
      "\u001b[34mdataset\u001b[m\u001b[m\n",
      "dataset.zip\n",
      "\u001b[34mdist_images\u001b[m\u001b[m\n",
      "dist_images.zip\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " X_train, Y_train, X_test, Y_test, classes = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will show you an image in the dataset. Feel free to change the index and re-run the cell multiple times to check out other images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a picture\n",
    "index = 3\n",
    "#classes = ['cat', 'dog']\n",
    "plt.imshow(X_train[index])\n",
    "plt.show()\n",
    "print (\"y = \" + str(Y_train[0,index]) + \". It's a \" + classes[Y_train[0,index]]  +  \" picture.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore your dataset \n",
    "m_train = X_train.shape[0]\n",
    "num_px = X_train.shape[1]\n",
    "m_test = X_test.shape[0]\n",
    "\n",
    "print (\"Number of training examples: \" + str(m_train))\n",
    "print (\"Number of testing examples: \" + str(m_test))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"Train_x shape: \" + str(X_train.shape))\n",
    "print (\"Train_y shape: \" + str(Y_train.shape))\n",
    "print (\"Test_x shape: \" + str(X_test.shape))\n",
    "print (\"Test_y shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, you reshape and standardise the images before feeding them to the network. The code is given in the cell below.\n",
    "\n",
    "<img src=\"images/imvector_dog_reshaped.png\" style=\"width:450px;height:300px;\">\n",
    "<caption><center><font color='purple'><b>Figure 1</b>: Image to vector conversion.</font></center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the training and test examples \n",
    "train_x_flatten = X_train.reshape(X_train.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = X_test.reshape(X_test.shape[0], -1).T\n",
    "\n",
    "# Standardise data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n",
    "\n",
    "train_y = Y_train;\n",
    "test_y = Y_test;\n",
    "print (\"train_x's shape: \" + str(train_x.shape))\n",
    "print (\"test_x's shape: \" + str(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "$12,288$ equals $64 \\times 64 \\times 3$, which is the size of one reshaped image vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3 - L-Layer Neural Network - Parameter Initialisation\n",
    "\n",
    "###  Test the L_layer_model with different parameter initialisations\n",
    "\n",
    "You'll use a L-layer neural network (already implemented for you). These are the initialisation methods you'll experiment with: \n",
    "- *Random initialisation* -- setting `initialisation = \"random\"` in the input argument. This initialises the weights to  random values.  \n",
    "- *Xavier initialisation* --  setting `initialisation = \"xavier\"` in the input argument. This initialises the weights to random values scaled according to a paper by Xavier (Glorot), 2010.\n",
    "- *He initialisation* -- setting `initialisation = \"he\"` in the input argument. This initialises the weights to random values scaled according to a paper by He et al., 2015. \n",
    "\n",
    "**Instructions**: Instructions: Read over the code below, and run it. In the next part, you'll implement the three initialisation methods that this `model()` calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bd68f4ac31e8b22bbf46fab0a7e0e28b",
     "grade": false,
     "grade_id": "cell-dd8ea98cb7dac175",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# L_layer_model\n",
    "\n",
    "def L_layer_model(X, Y, layer_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False, initialisation = \"xavier\"):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if dog), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimisation loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    initialisation -- flag to choose which initialisation to use (\"random\",\"xavier\" or \"he\")\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Initialise parameters dictionary.\n",
    "    if initialisation == \"random\":\n",
    "        parameters = initialise_parameters_random(layer_dims)\n",
    "    elif initialisation == \"xavier\":\n",
    "        parameters = initialise_parameters_xavier(layer_dims)\n",
    "    elif initialisation == \"he\":\n",
    "        parameters = initialise_parameters_he(layer_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X, parameters)        \n",
    "         \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL, Y)\n",
    "   \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    " \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                  \n",
    "        # Print the cost every 100 iterations\n",
    "        if print_cost and i % 100 == 0 or i == num_iterations - 1:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0 or i == num_iterations:\n",
    "            costs.append(cost)\n",
    "   \n",
    "    return parameters, costs\n",
    "\n",
    "def plot_costs(costs, learning_rate = 0.0075):\n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.plot(costs)   \n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundereds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3-1'></a>\n",
    "## 3.1 - Random Initialisation\n",
    "\n",
    "There are two types of parameters to initialise in a neural network:\n",
    "- The weight matrices $(W^{[1]}, W^{[2]}, W^{[3]}, ..., W^{[L-1]}, W^{[L]})$\n",
    "- The bias vectors $(b^{[1]}, b^{[2]}, b^{[3]}, ..., b^{[L-1]}, b^{[L]})$]\n",
    "\n",
    "Initialise the weights randomly so that each neuron can then proceed to learn a different function of its inputs. In this exercise, you'll see what happens when the weights are initialised randomly to large values.\n",
    "\n",
    "<a name='ex-1'></a>\n",
    "### Exercise 1.1 - initialise_parameters_random\n",
    "\n",
    "Implement the following function to initialise your weights to relatively large random values (scaled by \\*0.20) and your biases to zeros. Use `np.random.randn(..,..) * 0.2` for weights and `np.zeros((.., ..))` for biases. You're using a fixed `np.random.seed(..)` to make sure your \"random\" weights match the teaching assistants, so don't worry if running your code several times always gives you the same initial values for the parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialise_parameters_random\n",
    "\n",
    "def initialise_parameters_random(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L + 1):       \n",
    "        # ENTER CODE HERE (~ 2 lines of code)\n",
    "        # parameters['W' + str(l)] = \n",
    "        # parameters['b' + str(l)] =\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up your layer dimensions  for this exercise\n",
    "Run the cell below to set up as a 4-layer neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS ###\n",
    "layer_dims = [12288, 20, 7, 5, 1] #  4-layer model with 3 hidden layers and 1 output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train using random initialisation**\n",
    "- Train your model as a 4 layer neural network\n",
    "- The cost should decrease on every iteration. \n",
    "- It may take up to 5 minutes to run 2500 iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, costs = L_layer_model(train_x, train_y, layer_dims, learning_rate = 0.0075, num_iterations = 2500, print_cost = True, initialisation = \"random\")\n",
    "plot_costs(costs, learning_rate=0.0075)\n",
    "print (\"On the training set:\")\n",
    "pred_train = predict(train_x, train_y, parameters)\n",
    "print (\"On the test set:\")\n",
    "pred_test = predict(test_x, test_y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3-2'></a>\n",
    "## 3.2 - Xavier Initialisation\n",
    "\n",
    "Lets' try \"Xavier Initialisation\"; this is named for the first author of Xavier (Glorot), 2010. Xavier initialisation uses a scaling factor for the weights $W^{[l]}$ of `sqrt(1./layers_dims[l-1])`.\n",
    "\n",
    "<a name='ex-2'></a>\n",
    "### Exercise 1.2 - initialise_parameters_xavier\n",
    "\n",
    "Implement the following function to initialise your parameters with Xavier initialisation. This function is similar to the previous `initialise_parameters_random(...)`. The only difference is that instead of multiplying `np.random.randn(..,..)` by 0.20, you will multiply it by $\\sqrt{\\frac{1}{\\text{dimension of the previous layer}}}$. Xavier initialisation is recommended for layers with a sigmoid or tanh activation, but seems to work well in this DNN example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialise_parameters_xavier\n",
    "\n",
    "def initialise_parameters_xavier(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the size of each layer.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])\n",
    "                    b1 -- bias vector of shape (layers_dims[1], 1)\n",
    "                    ...\n",
    "                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n",
    "                    bL -- bias vector of shape (layers_dims[L], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) - 1 # integer representing the number of layers\n",
    "     \n",
    "    for l in range(1, L + 1):\n",
    "        # ENTER CODE HERE (~ 2 lines of code)\n",
    "        # parameters['W' + str(l)] = \n",
    "        # parameters['b' + str(l)] =        \n",
    "        \n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train using xavier initialisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, costs = L_layer_model(train_x, train_y, layer_dims, learning_rate = 0.0075, num_iterations = 2500, print_cost = True, initialisation = \"xavier\")\n",
    "plot_costs(costs, learning_rate=0.0075)\n",
    "print (\"On the training set:\")\n",
    "pred_train = predict(train_x, train_y, parameters)\n",
    "print (\"On the test set:\")\n",
    "pred_test = predict(test_x, test_y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3-3'></a>\n",
    "## 3.3 - He Initialisation\n",
    "\n",
    "Finally, lets' try \"He Initialisation\"; this is named for the first author of He et al., 2015. He initialisation uses `sqrt(2./layers_dims[l-1])`.)\n",
    "\n",
    "<a name='ex-3'></a>\n",
    "### Exercise 1.3 - initialise_parameters_he\n",
    "\n",
    "Implement the following function to initialise your parameters with He initialization. This function is similar to the previous `initialise_parameters_random(...)`. The only difference is that instead of multiplying `np.random.randn(..,..)` by 0.20, you will multiply it by $\\sqrt{\\frac{2}{\\text{dimension of the previous layer}}}$, which is the initialisation recommended for layers with a ReLU activation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialise_parameters_he\n",
    "\n",
    "def initialise_parameters_he(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the size of each layer.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])\n",
    "                    b1 -- bias vector of shape (layers_dims[1], 1)\n",
    "                    ...\n",
    "                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n",
    "                    bL -- bias vector of shape (layers_dims[L], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) - 1 # integer representing the number of layers\n",
    "     \n",
    "    for l in range(1, L + 1):\n",
    "        # ENTER CODE HERE (~ 2 lines of code)\n",
    "        # parameters['W' + str(l)] = \n",
    "        # parameters['b' + str(l)] =        \n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train using He initialisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, costs = L_layer_model(train_x, train_y, layer_dims, learning_rate = 0.0075, num_iterations = 2500, print_cost = True, initialisation = \"he\")\n",
    "plot_costs(costs, learning_rate=0.0075)\n",
    "print (\"On the training set:\")\n",
    "pred_train = predict(train_x, train_y, parameters)\n",
    "print (\"On the test set:\")\n",
    "pred_test = predict(test_x, test_y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color: red;'>Discuss the model results for these parameter iniatilisations.</span> ###\n",
    "\n",
    "**Comment ** on the model peformance using these initalisation methods. Is the DNN still underperforming and why?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert you commentary into this markdown cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now look at two techniques to reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - L-layer Neural Network with L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4-1'></a>\n",
    "## 4.1 - Regularized Model\n",
    "\n",
    "You will use the following neural network (already implemented for you below). This model can be used in *regularization mode* by:\n",
    "- setting the `lambd` input to a non-zero value. We use \"`lambd`\" instead of \"`lambda`\" because \"`lambda`\" is a reserved keyword in Python. \n",
    "\n",
    "You will implement:\n",
    "- *L2 regularization* -- functions: \"`compute_cost_with_regularization()`\" and \"`backward_propagation_with_regularization()`\"\n",
    "\n",
    "In each part, you will run this model with the correct inputs so that it calls the functions you've implemented. Take a look at the code below to familiarise yourself with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L_layer_model with L2 regularization - model()\n",
    "\n",
    "def L_layer_model_reg(X, Y, layer_dims, learning_rate = 0.0075, num_iterations = 2500, print_cost=True, initialisation = \"xavier\", lambd = 0):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if dog), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimisation loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    initialisation -- flag to choose which initialisation to use (\"random\",\"xavier\" or \"he\")\n",
    "    lambd -- regularization hyperparameter, scalar\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    m = X.shape[1]                     # number of training examples\n",
    "\n",
    "    # Initialise parameters dictionary.\n",
    "    if initialisation == \"random\":\n",
    "        parameters = initialise_parameters_random(layer_dims)\n",
    "    elif initialisation == \"xavier\":\n",
    "        parameters = initialise_parameters_xavier(layer_dims)\n",
    "    elif initialisation == \"he\":\n",
    "        parameters = initialise_parameters_he(layer_dims)\n",
    "           \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):    \n",
    "            \n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X, parameters)  \n",
    "    \n",
    "        # Compute cost.\n",
    "        if lambd == 0:\n",
    "            cost = compute_cost(AL, Y)\n",
    "        else:\n",
    "            cost = compute_cost_with_regularization(AL, Y, parameters, lambd)\n",
    "            \n",
    "        # Backward propagation.\n",
    "        if lambd == 0:\n",
    "            grads = L_model_backward(AL, Y, caches)\n",
    "        else:\n",
    "            grads = L_model_backward_with_regularization(AL, Y, caches, lambd)\n",
    "\n",
    "            \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                  \n",
    "        # Print the cost every 100 iterations\n",
    "        if print_cost and i % 100 == 0 or i == num_iterations - 1:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0 or i == num_iterations:\n",
    "            costs.append(cost)\n",
    "   \n",
    "    return parameters, costs\n",
    "\n",
    "def plot_costs(costs, learning_rate = 0.0075):\n",
    "    # plot the cost\n",
    "    #plt.plot(np.squeeze(costs))\n",
    "    plt.plot(costs)   \n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundereds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5-2'></a>\n",
    "## 5.2 - L2 Regularization\n",
    "\n",
    "The standard way to avoid overfitting is called **L2 regularization**. It consists of appropriately modifying your cost function, from:\n",
    "$$J = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small  y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} \\tag{1}$$\n",
    "To:\n",
    "$$J_{regularized} = \\small \\underbrace{-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} }_\\text{cross-entropy cost} + \\underbrace{\\frac{1}{m} \\frac{\\lambda}{2} \\sum\\limits_l\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2} }_\\text{L2 regularization cost} \\tag{2}$$\n",
    "\n",
    "Let's modify your cost and observe the consequences.\n",
    "\n",
    "<a name='ex-4'></a>\n",
    "### Exercise 2.1 - compute_cost_with_regularization\n",
    "Implement `compute_cost_with_regularization()` which computes the cost given by equation (2). To calculate $\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2}$  , use :\n",
    "```python\n",
    "np.sum(np.square(Wl))\n",
    "```\n",
    "Note that you have to do this for $W^{[1]}$, $W^{[2]}$,..., $W^{[L]}$, then sum the terms and multiply by $ \\frac{1}{m} \\frac{\\lambda}{2} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost_with_regularization\n",
    "\n",
    "def compute_cost_with_regularization(AL, Y, parameters, lambd):\n",
    "    \"\"\"\n",
    "    Implement the cost function with L2 regularization. See formula (2) above.\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- post-activation, output of forward propagation, of shape (output size, number of examples)\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "\n",
    "    Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
    "    parameters -- python dictionary containing parameters of the model\n",
    "    \n",
    "    Returns:\n",
    "    cost - value of the regularized loss function (formula (2))\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "    cross_entropy_cost = compute_cost(AL, Y) # This gives you the cross-entropy part of the cost\n",
    "       \n",
    "    # ENTER CODE HERE (2~4 lines of code)\n",
    "    # L2_regularization_cost =    \n",
    "    \n",
    "    \n",
    "    cost = cross_entropy_cost + L2_regularization_cost\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, because you changed the cost, you have to change backward propagation as well! All the gradients have to be computed with respect to this new cost. \n",
    "\n",
    "<a name='ex-5'></a>\n",
    "### Exercise 2.2 - linear_backward_with_regularization\n",
    "Implement the changes needed in backward propagation to take into account regularization. This function is called by the L_model_backward_with_regularization() function in the main model. The changes only concern dW1, dW2, ..., dWL. For each, you have to add the regularization term's gradient ($\\frac{d}{dW} ( \\frac{1}{2}\\frac{\\lambda}{m}  W^2) = \\frac{\\lambda}{m} W$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_backward_propagation_with_regularization\n",
    "\n",
    "def linear_backward_with_regularization(dZ, cache, lambd):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    # ENTER CODE HERE (~ 1 line of code)\n",
    "    # dW = 1./m * np.dot(dZ,A_prev.T) + ....    \n",
    "\n",
    "    \n",
    "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_dims = [12288, 20, 7, 5, 1] \n",
    "parameters, costs = L_layer_model_reg(train_x, train_y, layer_dims, initialisation = \"he\", lambd=0.8)\n",
    "\n",
    "plot_costs(costs, learning_rate=0.0075)\n",
    "print (\"On the training set:\")\n",
    "pred_train = predict(train_x, train_y, parameters)\n",
    "print (\"On the testset:\")\n",
    "pred_test = predict(test_x, test_y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**:\n",
    "- The value of $\\lambda$ is a hyperparameter that you can tune using a dev set.\n",
    "- L2 regularization makes your decision boundary smoother. If $\\lambda$ is too large, it is also possible to \"oversmooth\", resulting in a model with high bias.\n",
    "\n",
    "**What is L2-regularization actually doing?**:\n",
    "\n",
    "L2-regularization relies on the assumption that a model with small weights is simpler than a model with large weights. Thus, by penalising the square values of the weights in the cost function you drive all the weights to smaller values. It becomes too costly for the cost to have large weights! This leads to a smoother model in which the output changes more slowly as the input changes. \n",
    "\n",
    "<br>\n",
    "<font color='blue'>\n",
    "    \n",
    "**What you should remember:** the implications of L2-regularization on:\n",
    "- The cost computation:\n",
    "    - A regularization term is added to the cost.\n",
    "- The backpropagation function:\n",
    "    - There are extra terms in the gradients with respect to weight matrices.\n",
    "- Weights end up smaller (\"weight decay\"): \n",
    "    - Weights are pushed to smaller values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5'></a>\n",
    "## 5 - L Layer Neural Network with Dropout\n",
    "\n",
    "Finally, **dropout** is a widely used regularization technique that is specific to deep learning. \n",
    "**It randomly shuts down some neurons in each iteration.** Watch these two videos to see what this means!\n",
    "\n",
    "<!--\n",
    "To understand drop-out, consider this conversation with a friend:\n",
    "- Friend: \"Why do you need all these neurons to train your network and classify images?\". \n",
    "- You: \"Because each neuron contains a weight and can learn specific features/details/shape of an image. The more neurons I have, the more featurse my model learns!\"\n",
    "- Friend: \"I see, but are you sure that your neurons are learning different features and not all the same features?\"\n",
    "- You: \"Good point... Neurons in the same layer actually don't talk to each other. It should be definitly possible that they learn the same image features/shapes/forms/details... which would be redundant. There should be a solution.\"\n",
    "!--> \n",
    "\n",
    "\n",
    "<center>\n",
    "<video width=\"620\" height=\"440\" src=\"images/dropout1_kiank.mp4\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "</center>\n",
    "<br>\n",
    "<caption><center><font color='purple'><b>Figure 2 </b>: <b>Drop-out on the second hidden layer.</b> <br> At each iteration, you shut down (= set to zero) each neuron of a layer with probability $1 - keep\\_prob$ or keep it with probability $keep\\_prob$ (50% here). The dropped neurons don't contribute to the training in both the forward and backward propagations of the iteration. </font></center></caption>\n",
    "\n",
    "<center>\n",
    "<video width=\"620\" height=\"440\" src=\"images/dropout2_kiank.mp4\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "</center>\n",
    "\n",
    "<caption><center><font color='purple'><b>Figure 3</b>:<b> Drop-out on the first and third hidden layers. </b><br> $1^{st}$ layer: we shut down on average 40% of the neurons.  $3^{rd}$ layer: we shut down on average 20% of the neurons. </font></center></caption>\n",
    "\n",
    "\n",
    "When you shut some neurons down, you actually modify your model. The idea behind drop-out is that at each iteration, you train a different model that uses only a subset of your neurons. With dropout, your neurons thus become less sensitive to the activation of one other specific neuron, because that other neuron might be shut down at any time. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5-1'></a>\n",
    "## 5.1 - Dropout Model\n",
    "\n",
    "You will use the following neural network (already implemented for you below). This model can be used in *dropout mode* by:\n",
    "- setting the `keep_prob` list input to a value less than one for any layer\n",
    "\n",
    "You will implement:\n",
    "- *Dropout* -- functions: \"`drop_out_matrices()`\", \"`L_model_forward_with_dropout()`\" and \"`L_model_backward_with_dropout()`\"\n",
    "\n",
    "In each part, you will run this model with the correct inputs so that it calls the functions you've implemented. Take a look at the code below to familiarise yourself with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L_layer_model with drop-out\n",
    "\n",
    "def L_layer_model_with_do(X, Y, layer_dims, keep_prob, learning_rate = 0.0075, num_iterations = 2500, print_cost=True, initialisation = \"xavier\"):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layer_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    keep_prob - list of probabilities of keeping a neuron active during drop-out for each layer\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimisation loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    initialisation -- flag to choose which initialisation to use (\"random\",\"xavier\" or \"he\")\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    m = X.shape[1]                     # number of training examples\n",
    "\n",
    "    # Initialise parameters dictionary.\n",
    "    if initialisation == \"random\":\n",
    "        parameters = initialise_parameters_random(layer_dims)\n",
    "    elif initialisation == \"xavier\":\n",
    "        parameters = initialise_parameters_xavier(layer_dims)\n",
    "    elif initialisation == \"he\":\n",
    "        parameters = initialise_parameters_he(layer_dims)\n",
    "           \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "        # Initialise dropout matrices\n",
    "        D = drop_out_matrices(layer_dims, m, keep_prob)        \n",
    "            \n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward_with_dropout(X, parameters, D, keep_prob)\n",
    "            \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL, Y)\n",
    "            \n",
    "        # Backward propagation.\n",
    "        grads =  L_model_backward_with_dropout(AL, Y, caches, D, keep_prob)\n",
    "            \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                  \n",
    "        # Print the cost every 100 iterations\n",
    "        if print_cost and i % 100 == 0 or i == num_iterations - 1:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0 or i == num_iterations:\n",
    "            costs.append(cost)\n",
    "   \n",
    "    return parameters, costs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5-2'></a>\n",
    "### 5.2 - Forward Propagation with Dropout\n",
    "\n",
    "<a name='ex-6'></a>\n",
    "### Exercise 3.1- drop_out_matrices and L_model_forward_with_dropout\n",
    "\n",
    "Implement the forward propagation with dropout. You are using a L layer neural network, and will add dropout to all the layers using 2 function calls drop_out_matrices() and L_model_forward_with_dropout(). \n",
    "\n",
    "**Instructions**:\n",
    "You would like to shut down some neurons in the hidden layers. To do that, you are going to carry out 4 Steps:\n",
    "1. In the Q&A session, we dicussed creating a variable $d^{[1]}$ with the same shape as $a^{[1]}$ using `np.random.rand()` to randomly get numbers between 0 and 1. Here, you will use a vectorised implementation, so create a random matrix $D^{[1]} = [d^{[1](1)} d^{[1](2)} ... d^{[1](m)}] $ of the same dimension as $A^{[1]}$.You will create a $D^{[l]}$ matrix for each layer in the drop_out_matrices() function.\n",
    "2. Set each entry of $D^{[1..L]}$ to be 1 with probability (`keep_prob`), and 0 otherwise.\n",
    "\n",
    "**Hint:** Let's say that keep_prob = 0.8, which means that we want to keep about 80% of the neurons and drop out about 20% of them.  We want to generate a vector that has 1's and 0's, where about 80% of them are 1 and about 20% are 0.\n",
    "This python statement:  \n",
    "`X = (X < keep_prob).astype(int)`  \n",
    "\n",
    "is conceptually the same as this if-else statement (for the simple case of a one-dimensional array) :\n",
    "\n",
    "```\n",
    "for i,v in enumerate(x):\n",
    "    if v < keep_prob:\n",
    "        x[i] = 1\n",
    "    else: # v >= keep_prob\n",
    "        x[i] = 0\n",
    "```\n",
    "Note that the `X = (X < keep_prob).astype(int)` works with multi-dimensional arrays, and the resulting output preserves the dimensions of the input array.\n",
    "\n",
    "Also note that without using `.astype(int)`, the result is an array of booleans `True` and `False`, which Python automatically converts to 1 and 0 if we multiply it with numbers.  (However, it's better practice to convert data into the data type that we intend, so try using `.astype(int)`.)\n",
    "\n",
    "3. In the L_model_forward_with_dropout() function, you will set $A^{[l]}$ to $A^{[l]} * D^{[l]}$ for each layer. (You are shutting down some neurons). You can think of $D^{[l]}$ as a mask, so that when it is multiplied with another matrix, it shuts down some of the values.\n",
    "4. Divide $A^{[l]}$ by `keep_prob`. By doing this you are assuring that the result of the cost will still have the same expected value as without drop-out. (This technique is also called inverted dropout.)\n",
    "\n",
    "*Note that for the input layer X, you will set  $A^{[0]}$ to $X * D^{[0]}$ and in the final layer you will set $A^{[L]}$ to $A^{[L]} * D^{[L]}$*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: drop_out_matrices and L_model_forward_with_dropout\n",
    "\n",
    "def drop_out_matrices(layer_dims, m, keep_prob):\n",
    "    \"\"\"\n",
    "    Generate the dropout_matrices D[l,m]\n",
    "    \n",
    "    Arguments:\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    m -- number of training examples\n",
    "    keep_prob - list of probabilities of keeping a neuron active during drop-out for each layer  \n",
    "\n",
    "    Returns:\n",
    "    D - list containing the Dropout values for each layer of size (layer_dims[l], m)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    L = len(layer_dims)\n",
    "    D = {} # Returns D=D[0]D[1]D[2]D[3]D[4] for eg. with layer_dims =[12288, 20, 7, 5, 1]\n",
    "          \n",
    "    for l in range(L):      \n",
    "        # ENTER CODE HERE (~ 2 lines of code)\n",
    "        #  D[l] =    # Initialise the random values for the dropout matrix\n",
    "        #  D[l] =    # Convert it to 0/1 to shut down neurons corresponding to each element\n",
    "   \n",
    "        \n",
    "        assert(D[l].shape == (layer_dims[l], m))\n",
    "        \n",
    "    return D\n",
    "\n",
    "\n",
    "def L_model_forward_with_dropout(X, parameters, D, keep_prob):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialise_parameters_() python dictionary containing your parameters \"W1\",\"b1\"...\"WL\",\"bL\"\n",
    "    keep_prob - list of probabilities of keeping a neuron active during drop-out for each layer  \n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value,  output of the forward propagation, of shape (1,1)\n",
    "    caches -- tuple, information stored for computing the backward propagation\n",
    "                list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []  \n",
    "    L = len(parameters) // 2       # number of layers in the neural network\n",
    "    A = X                          # input matrix A0\n",
    "    \n",
    "    # ENTER CODE HERE (~ 2 lines of code)\n",
    "    # A =  # shut down some units \n",
    "    # A =  # scale that value of units to keep expected value the same A =\n",
    "\n",
    "\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n",
    "\n",
    "        # ENTER CODE HERE (~ 2 lines of code)\n",
    "        # A =    # shut down some units A\n",
    "        # A =    # scale that value of units to keep expected value the same  \n",
    "        \n",
    "        caches.append(cache)     \n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n",
    "\n",
    "    # ENTER CODE HERE (~ 2 lines of code)\n",
    "    # Al =   # shut down some units in last layer, Al=\n",
    "    # Al =   # scale that value of units to keep expected value the same  \n",
    "\n",
    "    \n",
    "    caches.append(cache)\n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='5-3'></a>\n",
    "### 5.3 - Backward Propagation with Dropout\n",
    "\n",
    "<a name='ex-7'></a>\n",
    "### Exercise 3.2 - L_model_backward_with_dropout\n",
    "Implement the backward propagation with dropout. As before, you are training a L layer network. Add dropout to the layers, using the masks $D^{[1]}$ ... $D^{[L]}$ generated from drop_out_matrices(). \n",
    "\n",
    "**Instruction**:\n",
    "Backpropagation with dropout is actually quite easy. You will have to carry out 2 Steps:\n",
    "1. You had previously shut down some neurons during forward propagation, by applying a mask $D^{[1]}$ to `A1`. In backpropagation, you will have to shut down the same neurons, by reapplying the same mask $D^{[1]}$ to `dA1`. \n",
    "2. During forward propagation, you had divided `A1` by `keep_prob`. In backpropagation, you'll therefore have to divide `dA1` by `keep_prob` again (the calculus interpretation is that if $A^{[1]}$ is scaled by `keep_prob`, then its derivative $dA^{[1]}$ is also scaled by the same `keep_prob`).\n",
    "3. Repeat steps 1 and 2 for all the layers in the network starting with the output layer computing $dA^{[L]}$ .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_backward_with_dropout\n",
    "\n",
    "def L_model_backward_with_dropout(AL, Y, caches, D, keep_prob):\n",
    "    \"\"\"\n",
    "    Implements the backward propagation of our baseline model to which we add dropout capbility.\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    X -- input dataset, of shape (input size, number of examples)\n",
    "    Y -- \"true\" labels vector, of shape (output size, number of examples)\n",
    "    caches -- cache output from forward_propagation()\n",
    "    keep_prob - list of probabilities of keeping a neuron active during drop-out for each layer\n",
    "\n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "      \n",
    "    # Initialising the backpropagation, dA for output layer\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "\n",
    "    # ENTER CODE HERE (~ 2 lines of code)\n",
    "    # dAL =    # shut down dAl =\n",
    "    # dAL =    # scale the value of units dAl=\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    current_cache = caches[L-1]\n",
    "    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\") \n",
    "\n",
    "    # ENTER CODE HERE (~ 2 lines of code)\n",
    "    # grads[\"dA\" + str(l - 1)] =   # shut down dA(L-1)\n",
    "    # grads[\"dA\" + str(l - 1)] =   # scale the value of units dA(L-1)  \n",
    "\n",
    "    grads[\"dW\" + str(L)] = dW_temp\n",
    "    grads[\"db\" + str(L)] = db_temp\n",
    "    \n",
    "    \n",
    "    for l in range(L-1, 0, -1):  # do l=L-1, .., l=1\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        current_cache = caches[l-1]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l)], current_cache, activation = \"relu\")\n",
    "\n",
    "        # ENTER CODE HERE (~ 2 lines of code)\n",
    "        # grads[\"dA\" + str(l - 1)] =   # shut down dA(l-1)\n",
    "        # grads[\"dA\" + str(l - 1)] =   # scale the value of units dA(l-1)=\n",
    "\n",
    "        \n",
    "        grads[\"dW\" + str(l)] = dW_temp\n",
    "        grads[\"db\" + str(l)] = db_temp        \n",
    "        \n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now run the model with dropout (`keep_prob = 0.86`). We can set a keep_prob value for each layer, however we will not use dropout for the input and final 2 layers (including the output layer). It means at every iteration you shut down each neurons of layer 1 and 2 with 14% probability. The function `model()` will now call:\n",
    "- `L_model_forward_with_dropout` instead of `L_model_forward`.\n",
    "- `L_model_backward_with_dropout` instead of `L_model_backward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup keep probabilities list for layers\n",
    "keep_prob = [1, 0.86, 0.86, 1, 1]\n",
    "layer_dims = [12288, 20, 7, 5, 1] #  4-layer model (input-layer, 3 hidden-layers + 1 output=layer)\n",
    "parameters, costs = L_layer_model_with_do(train_x, train_y, layer_dims, keep_prob=keep_prob, learning_rate=0.03, num_iterations=2500, initialisation = \"he\")\n",
    "\n",
    "plot_costs(costs, learning_rate=0.3)\n",
    "print (\"On the train set:\")\n",
    "pred_train = predict(train_x, train_y, parameters)\n",
    "print (\"On the test set:\")\n",
    "pred_test = predict(test_x, test_y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style='color: red;'>Discuss the model results using Dropout.</span> ###\n",
    "\n",
    "**Comment** on the model peformance using Dropout. Is the DNN performing differently and what observations can you make?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insert you commentary into this markdown cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='6'></a>\n",
    "##  6 - Results Analysis\n",
    "\n",
    "First, take a look at some images the L-layer model labeled incorrectly. This will show a few mislabeled images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_mislabeled_images(classes, test_x, test_y, pred_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A few types of images the model tends to do poorly on include:** \n",
    "- Animal body in an unusual position\n",
    "- Animal appears against a background of a similar color\n",
    "- Unusual animal color and species\n",
    "- Camera angle\n",
    "- Brightness of the picture\n",
    "- Scale variation (dog/cat is very large or small in image) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations on finishing this assignment! \n",
    "\n",
    "You just built and trained a deep L-layer neural network, and applied it in order to distinguish dogs from cats,a good introduction to deep learning. ;) \n",
    "\n",
    "Amazing work! If you'd like to test out further - there's an optional ungraded exercise below, where you can test your own image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='7'></a>\n",
    "## 7 - Test with your own image (optional/ungraded exercise) ##\n",
    "\n",
    "From this point, if you so choose, you can use your own image to test  the output of your model. To do that follow these steps:\n",
    "\n",
    "1. Click on \"File\" in the upper bar of this notebook, then click \"Open\" to go on your home directory.\n",
    "2. Add your image to this Jupyter Notebook's directory, in the \"images\" folder\n",
    "3. Change your image's name in the following code\n",
    "4. Run the code and check if the algorithm is right (0 = cat, 1 = dog)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MODIFY THIS CODE HERE ##\n",
    "my_image = \"my_image_cat.jpg\" # change this to the name of your image file \n",
    "my_label_y = [0] # the true class of your image (0 -> cat, 1 -> dog )\n",
    "## MODIFICATION ENDS HERE ##\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (5, 4) # set default size of plots\n",
    "fname = \"images/\" + my_image\n",
    "image = np.array(Image.open(fname).resize((num_px, num_px)))\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "         \n",
    "image = image / 255.\n",
    "image = image.reshape((1, num_px * num_px * 3)).T\n",
    "\n",
    "my_predicted_image = predict(image, my_label_y, parameters)\n",
    "print (\"y = \" + str(np.squeeze(my_predicted_image)) + \", your L-layer model predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image))] +  \"\\\" picture.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**:\n",
    "\n",
    "- Xavier Initialisation: https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\n",
    "- He Initialisation: https://arxiv.org/abs/1502.01852"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "TSPse",
   "launcher_item_id": "24mxX"
  },
  "kernelspec": {
   "display_name": "Python (cpu_env)",
   "language": "python",
   "name": "cpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
